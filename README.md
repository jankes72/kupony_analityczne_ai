# Kupony Analityczne AI

## Opis projektu

Projekt to praktyczny zestaw narzÄ™dzi do gromadzenia, transformacji i eksperymentÃ³w ML na danych meczowych
(koncentruje siÄ™ na hokeju). Repo zawiera pipeline od pobrania surowych danych, przez feature engineering,
generowanie wariantÃ³w syntetycznych, do prostego treningu modeli (TF / PyTorch) i przykÅ‚adÃ³w uÅ¼ycia.

GÅ‚Ã³wne moÅ¼liwoÅ›ci:
- integracja z API-Sports przez `app/sport_wrapper.py` â€” pobieranie meczÃ³w, eventÃ³w i statystyk oraz zapis do SQLite,
- budowa datasetÃ³w Parquet z gotowymi cechami i targetami (`/build-dataset`),
- generator danych syntetycznych (`app/generator_synthetic_data.py`) â€” augmentacja rekordÃ³w (zachowuje oryginaÅ‚ + warianty),
- przykÅ‚adowe helpery i demo treningu (`app/example.py`) â€” konwersja DF -> tensory, krÃ³tkie trenowanie i zapis modeli,
- serwer FastAPI (`app/main.py`) z endpointami operacyjnymi i integracjÄ… generatora syntetycznego.

## Charakterystyka

- ðŸ§­ Integracja z API-Sports (wrapper `ApiSportsHockey`) â€” pobieranie lig, meczÃ³w, eventÃ³w, standings,
- ðŸ—„ï¸ Lokalna baza SQLite do przechowywania zasysanych meczÃ³w (`hockey.sqlite`),
- ðŸ“¦ Eksport datasetÃ³w do Parquet z gotowymi cechami (`nhl_2023.parquet` przykÅ‚adowy plik),
- âš¡ FastAPI z endpointami: zdrowie, statystyki, monitoring systemu, endpointy operacyjne (`collect-world-data`, `fetch-and-store-season`, `build-dataset`),
- ðŸ§© Helpery do feature-engineering (`app/features_helpers.py`) â€” budowa targetÃ³w, normalizacja kursÃ³w, cechy formy i H2H,
- ðŸ› ï¸ CLI w `app/sport_wrapper.py` do szybkich testÃ³w (pobieranie lig, fetch, budowa datasetu),
- ðŸ“š Interaktywna dokumentacja OpenAPI dostÄ™pna pod `/docs` i `/redoc`.

## Wymagania

- Python 3.8+
- pip
- ZaleÅ¼noÅ›ci w `requirements.txt` (FastAPI, Uvicorn, pandas, requests, psutil, pydantic, itp.).

## Instalacja

1. Klonuj repozytorium
```bash
git clone https://github.com/jankes72/kupony_analityczne_ai.git
cd kupony_analityczne_ai
```

2. Zainstaluj zaleÅ¼noÅ›ci
```bash
pip install -r requirements.txt
```

## Uruchomienie

```bash
python run.py
```

Serwer uruchomi siÄ™ na `http://localhost:8000`.

## API Endpoints (szybki przeglÄ…d)

- `GET /` â€” informacje o API i lista endpointÃ³w
- `GET /health` â€” health-check
- `GET /stats` â€” przykÅ‚adowe statystyki aplikacji
- `GET /monitor` â€” metryki systemowe (CPU, pamiÄ™Ä‡, uptime)
- `GET /settings` â€” zwraca statyczne ustawienia
- `POST /collect-world-data` â€” uniwersalny proxy do `ApiSportsHockey`
- `POST /fetch-and-store-season` â€” zasysa mecze dla podanego `league`+`season` i zapisuje do SQLite
- `POST /build-dataset` â€” buduje dataset Parquet z danych w SQLite (feature engineering)

SzczegÃ³Å‚owe kontrakty request/response znajdujÄ… siÄ™ niÅ¼ej oraz w automatycznie wygenerowanej dokumentacji OpenAPI.

## Kontrakty endpointÃ³w (request / response)

### POST /collect-world-data

Request JSON (przykÅ‚ad):

```json
{
	"sports": {
		"api_key": "API_SPORTS_KEY",
		"action": "leagues|games|game|games_events|team_statistics",
		"params": { "league": 57, "season": 2024 }
	}
}
```

Response (przykÅ‚ad, zaleÅ¼y od akcji):

```json
{
	"result": [ /* array lub obiekt zwrÃ³cony przez ApiSports */ ]
}
```

### POST /fetch-and-store-season

Request JSON:

```json
{
	"api_key": "API_SPORTS_KEY",
	"league": 57,
	"season": 2024,
	"db_path": "./hockey.sqlite"
}
```

Response (przykÅ‚ad):

```json
{
	"ok": true,
	"summary": {
		"league": 57,
		"season": 2024,
		"fetched": 123,
		"db_path": "./hockey.sqlite"
	}
}
```

### POST /build-dataset

Request JSON (przykÅ‚ad):

```json
{
	"league": 57,
	"season": 2024,
	"db_path": "./hockey.sqlite",
	"output_path": "./nhl_2024.parquet",
	"return_file": false
}
```

Opis dziaÅ‚ania:
- Endpoint buduje podstawowy dataset z rekordÃ³w w SQLite (feature engineering, targety),
- nastÄ™pnie (opcjonalnie/konfigurowalnie) stosuje generator syntetyczny i dopisuje warianty do finalnego Parquet
  â€” w wyniku dostajesz oryginalne rekordy + wygenerowane warianty w `output_path`.

Response (przykÅ‚ady):

- Gdy `return_file` = `false`:

```json
{
	"ok": true,
	"parquet_path": "dataset_hockey_league57_season2024.parquet"
}
```

- Gdy `return_file` = `true` â€” endpoint zwraca plik Parquet jako download (`Content-Disposition`).

Uwaga: augmentacja syntetyczna moÅ¼e znaczÄ…co zwiÄ™kszyÄ‡ rozmiar datasetu â€” kontroluj parametry/limit.

W przypadku bÅ‚Ä™dÃ³w endpointy zwracajÄ… odpowiednie kody HTTP i pole `detail` z opisem.

## Struktura projektu

```
kupony_analityczne_ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                  # GÅ‚Ã³wna aplikacja FastAPI (endpointy i integracja generatora)
â”‚   â”œâ”€â”€ sport_wrapper.py         # Wrapper ApiSportsHockey, DB helpers, dataset builder i CLI
â”‚   â”œâ”€â”€ generator_synthetic_data.py  # Generator wariantÃ³w syntetycznych (hokej)
â”‚   â”œâ”€â”€ generator_syntetic_data.py   # (starsza/alternatywna wersja, moÅ¼e byÄ‡ zarchiwizowana)
â”‚   â”œâ”€â”€ example.py               # Helpery DF->tensory, krÃ³tkie treningi TF/PyTorch i demo
â”‚   â””â”€â”€ features_helpers.py      # Feature engineering helpers i GameRow dataclass
â”œâ”€â”€ models/                      # miejsce zapisu przykÅ‚adÃ³w wytrenowanych modeli
â”œâ”€â”€ run.py                       # Entry point uruchamiajÄ…cy Uvicorn
â”œâ”€â”€ requirements.txt             # ZaleÅ¼noÅ›ci
â”œâ”€â”€ .env.example                 # PrzykÅ‚ad zmiennych Å›rodowiskowych
â”œâ”€â”€ hockey.sqlite                # (opcjonalnie) przykÅ‚adowa baza danych SQLite
â””â”€â”€ nhl_2023.parquet             # (opcjonalnie) przykÅ‚adowy dataset Parquet
```

## Dataflow (end-to-end)

PoniÅ¼ej opis krokÃ³w przetwarzania danych i przepÅ‚ywu od pobrania surowych zdarzeÅ„ do wytrenowanego modelu:

1. Pobranie danych z API
	- `app/sport_wrapper.py` (klasa `ApiSportsHockey`) Å‚Ä…czy siÄ™ z API-Sports i pobiera mecze, eventy, statystyki.
	- Dane sÄ… zapisywane do lokalnej bazy SQLite (`hockey.sqlite`) â€” tabela meczÃ³w, events, teams, standings.

2. Budowa podstawowego datasetu (feature engineering)
	- Endpoint `POST /build-dataset` lub metoda `ApiSportsHockey.build_dataset()` czyta rekordy z SQLite,
	  wykonuje feature engineering (H2H, forma, normalizacje, targety) i zapisuje wynik do Parquet (np. `out.parquet`).

3. Augmentacja syntetyczna
	- Opcjonalnie (zintegrowane w `app/main.py`): dla kaÅ¼dego rekordu bazowego moÅ¼na wygenerowaÄ‡ warianty syntetyczne
	  przy uÅ¼yciu generatora (`app/generator_synthetic_data.py`).
	- Strategia: zachowujemy oryginalny rekord i dopisujemy wygenerowane warianty, zapisujÄ…c ostateczny dataset do Parquet.
	- Uwaga: eksplozja liczby wierszy â€” dla duÅ¼ych datasetÃ³w uÅ¼ywaj prÃ³bkowania lub strumieniowania/partycjonowania.

4. Przechowywanie datasetu
	- Finalny Parquet jest ÅºrÃ³dÅ‚em dla eksperymentÃ³w i treningu modeli (`nhl_YYYY.parquet` lub `out.parquet`).

5. Trening modelu
	- Skrypt `app/example.py` pokazuje jak przygotowaÄ‡ tensory (TensorFlow / PyTorch) z Parquet,
	  wykonaÄ‡ krÃ³tki trening i zapisaÄ‡ model do `models/`.
	- TensorFlow: zapis Keras (`models/tf_demo_model/`).
	- PyTorch: zapis wag (`models/torch_demo_model.pt`).

6. UÅ¼ycie modelu w produkcji / innym skrypcie
	- TensorFlow: `tf.keras.models.load_model("models/tf_demo_model")` i `model.predict(X_new)`.
	- PyTorch: utwÃ³rz sieÄ‡ (np. `build_torch_model`) i zaÅ‚aduj `model.load_state_dict(torch.load(path))`.

Praktyczne wskazÃ³wki:
- Zachowaj spÃ³jnÄ… listÄ™ cech (`FEATURE_COLS`) i stosuj to samo przetwarzanie przy treningu i predykcji.
- Dla duÅ¼ych zbiorÃ³w danych unikaj Å‚adowania caÅ‚ego Parquet do pamiÄ™ci â€” uÅ¼yj chunkÃ³w / Dask / strumieniowania.
- JeÅ›li uÅ¼ywasz augmentacji syntetycznej, kontroluj rozmiar wyjÅ›ciowego datasetu (sampling, limit na rekordy).

## Zmienne Å›rodowiskowe

Skopiuj `.env.example` na `.env` i dostosuj wartoÅ›ci:

```
DEBUG=True
HOST=0.0.0.0
PORT=8000
```

## Licencja

MIT

## UÅ¼ycie wytrenowanego modelu (szybka Å›ciÄ…ga)

Po uruchomieniu `app/example.py` w katalogu `models/` pojawiÄ… siÄ™ przykÅ‚adowe zapisy modeli:

- TensorFlow: `models/tf_demo_model/` (katalog z zapisanym modelem Keras)
- PyTorch: `models/torch_demo_model.pt` (pliki wag sieci)

KrÃ³tki przykÅ‚ad uÅ¼ycia (TensorFlow):

```python
import tensorflow as tf
model = tf.keras.models.load_model("models/tf_demo_model")
X_new = ...  # numpy array z cechami w tej samej kolejnoÅ›ci co FEATURE_COLS
pred = model.predict(X_new)
```

Dla PyTorch:

```python
import torch
from app.example import build_torch_model
model = build_torch_model(input_dim=3, n_classes=1)
model.load_state_dict(torch.load("models/torch_demo_model.pt"))
model.eval()
X_new = torch.tensor([[...]], dtype=torch.float32)
with torch.no_grad():
	out = model(X_new)
	prob = torch.sigmoid(out).item()
```

Uwaga: wejÅ›cie musi mieÄ‡ tÄ™ samÄ… kolejnoÅ›Ä‡ i skalowanie cech co podczas treningu. Zapisz `FEATURE_COLS` i uÅ¼ywaj go wszÄ™dzie.
