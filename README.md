# Kupony Analityczne AI

## Opis projektu

Projekt to eksperyment badawczy nad modelami predykcyjnymi dla zdarzeÅ„ sportowych zintegrowanymi z
instalacjÄ… danych i API. System Å‚Ä…czy:

- eksperymenty z rÃ³Å¼nymi architekturami sieci neuronowych do predykcji wynikÃ³w,
- moduÅ‚ integrujÄ…cy z zewnÄ™trznym API (API-Sports) do zbierania danych meczowych (`app/sport_wrapper.py`),
- warstwÄ™ przechowywania i transformacji danych (SQLite -> Parquet) oraz narzÄ™dzia do feature engineering,
- serwer HTTP oparty na FastAPI z endpointami do monitoringu, pobierania danych, budowania datasetÃ³w i testÃ³w.

CaÅ‚oÅ›Ä‡ pozwala na: zasysanie danych, tworzenie feature'Ã³w, budowÄ™ datasetÃ³w Parquet i szybkÄ… ocenÄ™ modeli.

## Charakterystyka

- ğŸ§­ Integracja z API-Sports (wrapper `ApiSportsHockey`) â€” pobieranie lig, meczÃ³w, eventÃ³w, standings,
- ğŸ—„ï¸ Lokalna baza SQLite do przechowywania zasysanych meczÃ³w (`hockey.sqlite`),
- ğŸ“¦ Eksport datasetÃ³w do Parquet z gotowymi cechami (`nhl_2023.parquet` przykÅ‚adowy plik),
- âš¡ FastAPI z endpointami: zdrowie, statystyki, monitoring systemu, endpointy operacyjne (`collect-world-data`, `fetch-and-store-season`, `build-dataset`),
- ğŸ§© Helpery do feature-engineering (`app/features_helpers.py`) â€” budowa targetÃ³w, normalizacja kursÃ³w, cechy formy i H2H,
- ğŸ› ï¸ CLI w `app/sport_wrapper.py` do szybkich testÃ³w (pobieranie lig, fetch, budowa datasetu),
- ğŸ“š Interaktywna dokumentacja OpenAPI dostÄ™pna pod `/docs` i `/redoc`.

## Wymagania

- Python 3.8+
- pip
- ZaleÅ¼noÅ›ci w `requirements.txt` (FastAPI, Uvicorn, pandas, requests, psutil, pydantic, itp.).

## Instalacja

1. Klonuj repozytorium
```bash
git clone https://github.com/jankes72/kupony_analityczne_ai.git
cd kupony_analityczne_ai
```

2. Zainstaluj zaleÅ¼noÅ›ci
```bash
pip install -r requirements.txt
```

## Uruchomienie

```bash
python run.py
```

Serwer uruchomi siÄ™ na `http://localhost:8000`.

## API Endpoints (szybki przeglÄ…d)

- `GET /` â€” informacje o API i lista endpointÃ³w
- `GET /health` â€” health-check
- `GET /stats` â€” przykÅ‚adowe statystyki aplikacji
- `GET /monitor` â€” metryki systemowe (CPU, pamiÄ™Ä‡, uptime)
- `GET /settings` â€” zwraca statyczne ustawienia
- `POST /collect-world-data` â€” uniwersalny proxy do `ApiSportsHockey`
- `POST /fetch-and-store-season` â€” zasysa mecze dla podanego `league`+`season` i zapisuje do SQLite
- `POST /build-dataset` â€” buduje dataset Parquet z danych w SQLite (feature engineering)

SzczegÃ³Å‚owe kontrakty request/response znajdujÄ… siÄ™ niÅ¼ej oraz w automatycznie wygenerowanej dokumentacji OpenAPI.

## Kontrakty endpointÃ³w (request / response)

### POST /collect-world-data

Request JSON (przykÅ‚ad):

```json
{
	"sports": {
		"api_key": "API_SPORTS_KEY",
		"action": "leagues|games|game|games_events|team_statistics",
		"params": { "league": 57, "season": 2024 }
	}
}
```

Response (przykÅ‚ad, zaleÅ¼y od akcji):

```json
{
	"result": [ /* array lub obiekt zwrÃ³cony przez ApiSports */ ]
}
```

### POST /fetch-and-store-season

Request JSON:

```json
{
	"api_key": "API_SPORTS_KEY",
	"league": 57,
	"season": 2024,
	"db_path": "./hockey.sqlite"
}
```

Response (przykÅ‚ad):

```json
{
	"ok": true,
	"summary": {
		"league": 57,
		"season": 2024,
		"fetched": 123,
		"db_path": "./hockey.sqlite"
	}
}
```

### POST /build-dataset

Request JSON:

```json
{
	"league": 57,
	"season": 2024,
	"db_path": "./hockey.sqlite",
	"output_path": "./nhl_2024.parquet",
	"return_file": false
}
```

Response (przykÅ‚ady):

- Gdy `return_file` = `false`:

```json
{
	"ok": true,
	"parquet_path": "dataset_hockey_league57_season2024.parquet"
}
```

- Gdy `return_file` = `true` â€” endpoint zwraca plik Parquet jako download (`Content-Disposition`).

Uwagi: w przypadku bÅ‚Ä™dÃ³w endpointy zwracajÄ… odpowiednie kody HTTP i pole `detail` z opisem.

## Struktura projektu

```
kupony_analityczne_ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # GÅ‚Ã³wna aplikacja FastAPI (endpointy i modele Pydantic)
â”‚   â”œâ”€â”€ sport_wrapper.py     # Wrapper ApiSportsHockey, DB helpers, dataset builder i CLI
+â”‚   â”œâ”€â”€ features_helpers.py  # Feature engineering helpers i GameRow dataclass
+â”‚   â””â”€â”€ README.md            # Dokumentacja moduÅ‚Ã³w wewnÄ…trz `app/`
â”œâ”€â”€ run.py                   # Entry point uruchamiajÄ…cy Uvicorn
â”œâ”€â”€ requirements.txt         # ZaleÅ¼noÅ›ci
â”œâ”€â”€ .env.example             # PrzykÅ‚ad zmiennych Å›rodowiskowych
â”œâ”€â”€ hockey.sqlite            # (opcjonalnie) przykÅ‚adowa baza danych SQLite
â””â”€â”€ nhl_2023.parquet         # (opcjonalnie) przykÅ‚adowy dataset Parquet
```

## Dataflow (end-to-end)

PoniÅ¼ej opis krokÃ³w przetwarzania danych i przepÅ‚ywu od pobrania surowych zdarzeÅ„ do wytrenowanego modelu:

1. Pobranie danych z API
	- `app/sport_wrapper.py` (klasa `ApiSportsHockey`) Å‚Ä…czy siÄ™ z API-Sports i pobiera mecze, eventy, statystyki.
	- Dane sÄ… zapisywane do lokalnej bazy SQLite (`hockey.sqlite`) â€” tabela meczÃ³w, events, teams, standings.

2. Budowa podstawowego datasetu (feature engineering)
	- Endpoint `POST /build-dataset` lub metoda `ApiSportsHockey.build_dataset()` czyta rekordy z SQLite,
	  wykonuje feature engineering (H2H, forma, normalizacje, targety) i zapisuje wynik do Parquet (np. `out.parquet`).

3. Augmentacja syntetyczna
	- Opcjonalnie (zintegrowane w `app/main.py`): dla kaÅ¼dego rekordu bazowego moÅ¼na wygenerowaÄ‡ warianty syntetyczne
	  przy uÅ¼yciu generatora (`app/generator_synthetic_data.py`).
	- Strategia: zachowujemy oryginalny rekord i dopisujemy wygenerowane warianty, zapisujÄ…c ostateczny dataset do Parquet.
	- Uwaga: eksplozja liczby wierszy â€” dla duÅ¼ych datasetÃ³w uÅ¼ywaj prÃ³bkowania lub strumieniowania/partycjonowania.

4. Przechowywanie datasetu
	- Finalny Parquet jest ÅºrÃ³dÅ‚em dla eksperymentÃ³w i treningu modeli (`nhl_YYYY.parquet` lub `out.parquet`).

5. Trening modelu
	- Skrypt `app/example.py` pokazuje jak przygotowaÄ‡ tensory (TensorFlow / PyTorch) z Parquet,
	  wykonaÄ‡ krÃ³tki trening i zapisaÄ‡ model do `models/`.
	- TensorFlow: zapis Keras (`models/tf_demo_model/`).
	- PyTorch: zapis wag (`models/torch_demo_model.pt`).

6. UÅ¼ycie modelu w produkcji / innym skrypcie
	- TensorFlow: `tf.keras.models.load_model("models/tf_demo_model")` i `model.predict(X_new)`.
	- PyTorch: utwÃ³rz sieÄ‡ (np. `build_torch_model`) i zaÅ‚aduj `model.load_state_dict(torch.load(path))`.

Praktyczne wskazÃ³wki:
- Zachowaj spÃ³jnÄ… listÄ™ cech (`FEATURE_COLS`) i stosuj to samo przetwarzanie przy treningu i predykcji.
- Dla duÅ¼ych zbiorÃ³w danych unikaj Å‚adowania caÅ‚ego Parquet do pamiÄ™ci â€” uÅ¼yj chunkÃ³w / Dask / strumieniowania.
- JeÅ›li uÅ¼ywasz augmentacji syntetycznej, kontroluj rozmiar wyjÅ›ciowego datasetu (sampling, limit na rekordy).

## Zmienne Å›rodowiskowe

Skopiuj `.env.example` na `.env` i dostosuj wartoÅ›ci:

```
DEBUG=True
HOST=0.0.0.0
PORT=8000
```

## Licencja

MIT

## UÅ¼ycie wytrenowanego modelu (szybka Å›ciÄ…ga)

Po uruchomieniu `app/example.py` w katalogu `models/` pojawiÄ… siÄ™ przykÅ‚adowe zapisy modeli:

- TensorFlow: `models/tf_demo_model/` (katalog z zapisanym modelem Keras)
- PyTorch: `models/torch_demo_model.pt` (pliki wag sieci)

KrÃ³tki przykÅ‚ad uÅ¼ycia (TensorFlow):

```python
import tensorflow as tf
model = tf.keras.models.load_model("models/tf_demo_model")
X_new = ...  # numpy array z cechami w tej samej kolejnoÅ›ci co FEATURE_COLS
pred = model.predict(X_new)
```

Dla PyTorch:

```python
import torch
from app.example import build_torch_model
model = build_torch_model(input_dim=3, n_classes=1)
model.load_state_dict(torch.load("models/torch_demo_model.pt"))
model.eval()
X_new = torch.tensor([[...]], dtype=torch.float32)
with torch.no_grad():
	out = model(X_new)
	prob = torch.sigmoid(out).item()
```

Uwaga: wejÅ›cie musi mieÄ‡ tÄ™ samÄ… kolejnoÅ›Ä‡ i skalowanie cech co podczas treningu. Zapisz `FEATURE_COLS` i uÅ¼ywaj go wszÄ™dzie.
